{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cdb3cb",
   "metadata": {},
   "source": [
    "# BM25 Evaluation for Pira\n",
    "\n",
    "This Jupyter notebook evaluates the performance of BM25 retriever model on Pirá Dataset. \n",
    "\n",
    "The code is based on BM25 Haystack Library implementation: https://haystack.deepset.ai/overview/intro\n",
    "\n",
    "Check the full Pira GitHub at: https://github.com/C4AI/Pira"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00b3db",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from haystack.utils import launch_es\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset information\n",
    "\n",
    "Here we set some values necessary to load the dataset\n",
    "\n",
    "PATH_BASE -> Dataset path\n",
    "\n",
    "SUPPORTING_TEXT_COLUMN -> Indicates the Supporting Text Column. Use \"10\" for English or \"-2\" for Portuguese.\n",
    "\n",
    "ANSWER_COLUMN -> Indicates the Answer Column. Use \"6\" for English or \"7\" for Portuguese.\n",
    "\n",
    "QUESTION_COLUMN -> Indicates the Answer Column. Use \"2\" for English or \"3\" for Portuguese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNT45Lr7mi9h"
   },
   "outputs": [],
   "source": [
    "PATH_BASE = './Data/test.csv'\n",
    "\n",
    "ABSTRACT_COLUMN = 18\n",
    "ANSWER_COLUMN = 7\n",
    "QUESTION_COLUMN = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the index name for the document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_KNOWLEDGE_BASE = \"abstracts_100_pt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "It is important to ensure that we do not use the same supporting text more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pira_train = pd.read_csv(PATH_BASE + \"train.csv\").values.tolist()\n",
    "pira_val = pd.read_csv(PATH_BASE + \"validation.csv\").values.tolist()\n",
    "pira_test = pd.read_csv(PATH_BASE + \"test.csv\").values.tolist()\n",
    "\n",
    "pira_dataset = pira_train + pira_val + pira_test\n",
    "\n",
    "abstracts = []\n",
    "temp = []\n",
    "for i in range(len(pira_dataset)):\n",
    "    if pira_dataset[i][ABSTRACT_COLUMN] not in temp:\n",
    "        abstracts.append([pira_dataset[i][ABSTRACT_COLUMN], len(abstracts)+1])\n",
    "        temp.append(pira_dataset[i][ABSTRACT_COLUMN])\n",
    "del temp \n",
    " \n",
    "for i in range(len(pira_dataset)):\n",
    "    for j in range(len(abstracts)):\n",
    "        if pira_dataset[i][ABSTRACT_COLUMN] == abstracts[j][0]:\n",
    "            pira_dataset[i].append(abstracts[j][1])\n",
    "            \n",
    "dicts = []\n",
    "for line in abstracts:\n",
    "    dicts.append({'content' : line[0], 'meta' : {'idarticle': line[1]}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing ElasticSearch\n",
    "\n",
    "To Download ElasticSearch files, uncoment top lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGrpGy0KDhDb"
   },
   "outputs": [],
   "source": [
    "#! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
    "#! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
    "#! sudo chown -R daemon:daemon elasticsearch-7.9.2\n",
    "\n",
    "\n",
    "launch_es()\n",
    "\n",
    "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "\n",
    "# wait until ES has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the document store and writing supporting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1621624557643,
     "user": {
      "displayName": "Marcos Menon Jose",
      "photoUrl": "",
      "userId": "06041167050429695478"
     },
     "user_tz": 180
    },
    "id": "_MiLIWxiDj19",
    "outputId": "de8890d9-6825-4f05-ef65-b4ab39bcb68a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=INDEX_KNOWLEDGE_BASE)\n",
    "document_store.write_documents(docs,batch_size=1000)\n",
    "\n",
    "#document_store.delete_all_documents # Deleting documents if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Retriever Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ElasticsearchRetriever(document_store=document_store)\n",
    "\n",
    "\n",
    "pipeline = DocumentSearchPipeline(retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"O que é o Pré-Sal ?\"\n",
    "\n",
    "\n",
    "result = pipeline.run(\n",
    "    query=question,\n",
    "    params={\n",
    "        \"Retriever\": {\n",
    "            \"top_k\": 5,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function that returns the accuracy for a giving k\n",
    "\n",
    "\n",
    "This function checks for each question if the supporting text was one of the top k retrieved documents and generates the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_BM25_acc(questions, K_Values):\n",
    "    maxK = max(K_Values)\n",
    "    cont = 0\n",
    "    ids = []\n",
    "    for line in questions:\n",
    "        ids.append([])\n",
    "        result = pipeline.run(query=line[QUESTION_COLUMN],params={\"Retriever\": {\"top_k\": maxK}})\n",
    "        document_dict = ast.literal_eval(str(result[\"documents\"]).replace(\"<Document: \",\"\").replace(\"'}>\",\"'}\"))\n",
    "        for i in range(len(document_dict)):\n",
    "            ids[cont].append(int(document_dict[i][\"meta\"][\"idarticle\"]))\n",
    "        cont+=1\n",
    "    corrects = []\n",
    "    accuracies = []\n",
    "    for j in range(len(K_Values)):\n",
    "        corrects.append(0)\n",
    "        cont = 0\n",
    "        for line in questions:\n",
    "            if int(line[len(line)-1]) in ids[cont][:K_Values[j]]:\n",
    "                corrects[j]+=1\n",
    "            cont+=1\n",
    "        accuracies.append(corrects[j]/len(questions))\n",
    "    return(accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating BM25 performance for multiple k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "Ks = range(1,101)\n",
    "pira_test2 = pd.DataFrame(pira_test)\n",
    "test = pira_test2.dropna(subset=[pira_test2.columns[QUESTION_COLUMN]]).values.tolist()\n",
    "accs = get_BM25_acc( test, Ks)\n",
    "for i in range(len(Ks)):\n",
    "    print(\"accuracy for K = \" + str(Ks[i]) + \" -- is =\" + str(accs[i]))\n",
    "\n",
    "df_accs = pd.DataFrame(accs)\n",
    "df_accs.to_csv(PATH_SAVE_BM25_EVAL)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Haystack data prep bm25.ipynb",
   "provenance": [
    {
     "file_id": "10r7O6VkA5Ej6RBgeboFFvGrS7MdEMMFv",
     "timestamp": 1621628528806
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
