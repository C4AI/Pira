{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xNT45Lr7mi9h"
   },
   "outputs": [],
   "source": [
    "PATH_BASE = 'Data/'\n",
    "INDEX_KNOWLEDGE_BASE = \"abstracts_full_en\"\n",
    "DO_PREPROCESSING = False\n",
    "ABSTRACT_COLUMN = 10\n",
    "ANSWER_COLUMN = 6\n",
    "QUESTION_COLUMN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opDV9WHuD5u1",
    "outputId": "bce7fb56-109a-4a78-c0d7-e9742509407c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "675"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pira_train = pd.read_csv(PATH_BASE + \"train.csv\").values.tolist()\n",
    "pira_test = pd.read_csv(PATH_BASE + \"test.csv\").values.tolist()\n",
    "pira_val = pd.read_csv(PATH_BASE + \"validation.csv\").values.tolist()\n",
    "\n",
    "pira_dataset = pira_train+pira_test+pira_val\n",
    "\n",
    "abstracts = []\n",
    "temp = []\n",
    "for i in range(len(pira_dataset)):\n",
    "    if pira_dataset[i][10] not in temp:\n",
    "        abstracts.append([pira_dataset[i][10], len(abstracts)+1])\n",
    "        temp.append(pira_dataset[i][10])\n",
    "del temp \n",
    " \n",
    "for i in range(len(pira_dataset)):\n",
    "    for j in range(len(abstracts)):\n",
    "        if pira_dataset[i][10] == abstracts[j][0]:\n",
    "            pira_dataset[i].append(abstracts[j][1])\n",
    "            \n",
    "dicts = []\n",
    "for line in abstracts:\n",
    "    dicts.append({'content' : line[0], 'meta' : {'idarticle': line[1]}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n",
      "WARNING - haystack -  Object 'PreProcessor' is imported through a deprecated path. Please check out the docs for the new import path.\n",
      "WARNING - haystack -  Object 'PreProcessor' is imported through a deprecated path. Please check out the docs for the new import path.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from haystack.preprocessor import PreProcessor\n",
    "if DO_PREPROCESSING:\n",
    "\n",
    "    processor = PreProcessor(\n",
    "        clean_empty_lines=True,\n",
    "        clean_whitespace=True,\n",
    "        clean_header_footer=True,\n",
    "        split_by=\"word\",\n",
    "        split_length=NUMBER_OF_WORDS,\n",
    "        split_respect_sentence_boundary=False,\n",
    "        split_overlap=0\n",
    "    )\n",
    "    docs = []\n",
    "    cont=0\n",
    "\n",
    "    for dict1 in dicts:\n",
    "        if cont %100 ==0:\n",
    "            print(cont)\n",
    "        cont+=1\n",
    "        doc = processor.process(dict1)\n",
    "        docs = docs+doc\n",
    "else:\n",
    "    docs = dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(\"faiss_document_store.db\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1621624557643,
     "user": {
      "displayName": "Marcos Menon Jose",
      "photoUrl": "",
      "userId": "06041167050429695478"
     },
     "user_tz": 180
    },
    "id": "_MiLIWxiDj19",
    "outputId": "de8890d9-6825-4f05-ef65-b4ab39bcb68a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - haystack -  Object 'FAISSDocumentStore' is imported through a deprecated path. Please check out the docs for the new import path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c75e4b860b14f93afc8747a0983215b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing Documents:   0%|          | 0/675 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0, CUDA:1\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 2\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-question_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-question_encoder-single-nq-base\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-ctx_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-ctx_encoder-single-nq-base\n",
      "INFO - haystack.document_stores.faiss -  Updating embeddings for 675 docs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f377cb185048d1b04c766c506f5211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating Embedding:   0%|          | 0/675 [00:00<?, ? docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.9/site-packages/haystack/modeling/data_handler/dataset.py:65: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180487213/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  cur_tensor = torch.tensor([sample[t_name] for sample in features], dtype=torch.long)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/688 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from haystack.document_store import FAISSDocumentStore\n",
    "\n",
    "document_store = FAISSDocumentStore(similarity=\"dot_product\", index=INDEX_KNOWLEDGE_BASE)\n",
    "\n",
    "document_store.write_documents(docs)\n",
    "\n",
    "\n",
    "from haystack.retriever.dense import DensePassageRetriever\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    ")\n",
    "# Important: \n",
    "# Now that after we have the DPR initialized, we need to call update_embeddings() to iterate over all\n",
    "# previously indexed documents and update their embedding representation. \n",
    "# While this can be a time consuming operation (depending on corpus size), it only needs to be done once. \n",
    "# At query time, we only need to embed the query and compare it the existing doc embeddings which is very fast.\n",
    "document_store.update_embeddings(retriever)\n",
    "\n",
    "document_store.save(\"DPR_Store/DPR_Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "\n",
    "pipeline = DocumentSearchPipeline(retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Brazi pre salt ?\"\n",
    "\n",
    "\n",
    "result = pipeline.run(\n",
    "    query=question,\n",
    "    params={\n",
    "        \"Retriever\": {\n",
    "            \"top_k\": 5,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [<Document: {'content': 'Pre-salt carbonate reservoirs are located offshore Brazil at Santos, Campos and EspÃ­rito Santo Basins, reaching depths up to 7000 m (22966 ft) and water depths of approximately 2200 m (7218 ft). Most of those reservoirs present very high permeability, resulting in extraordinary oil production rates, imposing, consequently, an onerous task on injector wells to maintain pressure and reservoir mass balance. Part of these injector wells are located in scenarios of inferior permo-porous characteristic and still have their injection rates limited in order to avoid fracture propagation in the reservoirs. To improve the geomechanical models, a series of field tests were carried out to estimate minimum in situ stress and fracture propagation pressure in both reservoir and cap rock. The tests results allowed a change in the criterion for definition of the maximum injection pressure in injector wells. The objective of this work is to present the tests results, how they were performed and the operational problems faced, always seeking the shortest rig time. These results favored the understanding of the reservoir and cap rock behavior.', 'content_type': 'text', 'score': 0.6726359635809389, 'meta': {'idarticle': '324', 'vector_id': '397'}, 'embedding': None, 'id': '9b7a0a64b729a62bc8f7d95d1f6c8223'}>,\n",
       "  <Document: {'content': 'The Brazilian Pre-Salt region has garnered much attention since the first major discoveries were announced by Petrobras, in 2006. Since then, discoveries in the region have ranked among the largest in the world in the last ten years, including Tupi, Iara, and Libra. This led to various estimates, from within and outside the government, that mentioned extremely large total possible accumulations, ranging from 50 to over 300 billion barrels of recoverable oil in the \"Picanha Azul\" region, an area of approximately 150 thousand square kilometers. Since none of these estimates was accompanied by technical data, or a description of the methodology used, an assessment of the potential for yet-to-find oil was carried out using a software tool which models the exploration process, making Monte Carlo simulations based on the information available regarding wells drilled, discoveries made, respective dates, and the areas involved. The assessment methodology used has been successfully applied by the authors in evaluating the potential of other areas, such as the shallow waters of the Campos Basin. While the present Pre-Salt region assessment does not have as many information points available as in those prior assessments, the intense exploration activity has resulted in enough information being available about the region so as to be able conduct an assessment over the entire area. Applying the concepts of discovery sequence and field size distribution, and making geological estimates for parameters of the number and size of accumulations, the current assessment furnished a probability distribution for the number and expected size of individual accumulations (fields) yet to be discovered, as well as for the total accumulation of yet-to-find recoverable oil in the region. The relatively large range of possible values of the results reflects the lack of more exploratory experience (discoveries or dry wells), which will only come about with time, despite the intense exploratory efforts currently under way. Even so, it was possible to suggest that within probabilistic confidence levels of 95% and 5%, field sizes expected will range from 165 million barrels to eight billion barrels, and total accumulations will range from 115 billion barrels to over 288 billion barrels.', 'content_type': 'text', 'score': 0.6641699656160927, 'meta': {'idarticle': '627', 'vector_id': '101'}, 'embedding': None, 'id': '365dc874394940ddeef195f62b4b3f67'}>,\n",
       "  <Document: {'content': 'Located in the Brazilian continental shelf, the so called \"Pre-Salt\" reservoirs with large accumulations of excellent quality, high commercial value light oil have become the most strategic oil production site for Brazil. One of the main challenges that has recently arisen concerns the definition of an adequate process to address the high concentration of carbon dioxide (CO2) presented in their geological formations. For the Pre-Salt basin, several options of offshore carbon dioxide storage have been studied, such as: aquifers, depleted reservoirs, and salt caverns. Although a mined offshore salt cavern has never been constructed, its adoption has been gaining momentum due to the mantle origin of CO2, the thickness of rock layer above the salt layer, and the lack of reservoirs below the carbonate reservoirs capable to store gases. Because of its unprecedent characteristics, the development of such facility implies a series of challenges in terms of safety and integrity aspects. Using tailored qualitative risk management tools, this paper explores the many aspects and interdependencies of well design, well clustering configuration, drilling, solution mining, CO2 disposal, and abandonment phases related to offshore salt caverns. The results obtained so far indicated that, technically, they are a feasible solution for Pre-Salt CO2 storage. However, economic and environmental viability strongly depends on well clustering configuration and solution mining process.', 'content_type': 'text', 'score': 0.6623295064053978, 'meta': {'idarticle': '602', 'vector_id': '211'}, 'embedding': None, 'id': '5aca081daf7728d0b5da609df59736c1'}>,\n",
       "  <Document: {'content': 'The objective of this paper is to present how Petrobras is successfully managing production losses due to mineral scale formation in subsea production wells from its biggest offshore field1. Marlim Field, discovered in 1985 with a STOIIP estimated at 1,012 million STD m3 (6,369 million STB) and a field area of 146 km2 is located in water depths ranging from 600 m to 1100 m.Current Marlim Field production, around 446,754 bpd is supported by injecting 761,971 bpd of sea water. The water production is 217,150 bpd (water cut = 32%) and GOR is 82 STDm3/STDm3. A total of 117 wells are on operation, with 73 producers and 44 water injectors. The field was developed using subsea completion through the vertical, deviated and horizontal wells, equipped with cased hole and open hole gravel packed screens. Scale formation has occurred as a consequence of the incompatibility between the barium and strontium present in formation water and the high amount of sulfate in the injected seawater. To avoid production losses a plan of water management was implemented, including frequent produced water chemical analyses to investigate the convenience for applying a chemical bullhead treatment to remove scale in the production wells (tubing, screen, gravel pack and near well bore)2,3,4. This paper presents results from dissolver treatments performed in the field and also from a special investigation using a rig in a horizontal well. Valuable information was obtained in this intervention, clearing up the scaling occurrence phenomena in this scenario, allowing therefore an improvement on productivity recovery of the wells.', 'content_type': 'text', 'score': 0.6621784816478716, 'meta': {'idarticle': '104', 'vector_id': '543'}, 'embedding': None, 'id': 'cfbb2255ba16c07ccbe1f8c4bfd721c5'}>,\n",
       "  <Document: {'content': 'The \"Pre-salt application\" offers some unique and challenging difficulties for producers and the service companies who support their operations. The carbonate reservoirs which occur in Brazilian deepwater fields provide unique challenges that relate to high temperatures, the high H2S content, as well as severe saline and scaling conditions. It is quite common to find oil and gas fields with estimated H2S level in the produced gas between 100 and 200 ppmv and salinity approximately 230,000 mg/L. Hydrogen sulfide is a poisonous gas, very harmful to life, and removal is essential to comply with sulfur emissions, as well as to ensure system and pipeline integrity.', 'content_type': 'text', 'score': 0.6618254838322625, 'meta': {'idarticle': '493', 'vector_id': '172'}, 'embedding': None, 'id': '516189adb101ba44464d57466e34aaba'}>],\n",
       " 'root_node': 'Query',\n",
       " 'params': {'Retriever': {'top_k': 5}},\n",
       " 'query': 'Brazi pre salt ?',\n",
       " 'node_id': 'Retriever'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for K = 1 -- is =0.3656387665198238\n",
      "accuracy for K = 2 -- is =0.44933920704845814\n",
      "accuracy for K = 3 -- is =0.4845814977973568\n",
      "accuracy for K = 4 -- is =0.5242290748898678\n",
      "accuracy for K = 5 -- is =0.5550660792951542\n",
      "accuracy for K = 6 -- is =0.5859030837004405\n",
      "accuracy for K = 7 -- is =0.5903083700440529\n",
      "accuracy for K = 8 -- is =0.5947136563876652\n",
      "accuracy for K = 9 -- is =0.6299559471365639\n",
      "accuracy for K = 10 -- is =0.6475770925110133\n",
      "accuracy for K = 11 -- is =0.6519823788546255\n",
      "accuracy for K = 12 -- is =0.6563876651982379\n",
      "accuracy for K = 13 -- is =0.6740088105726872\n",
      "accuracy for K = 14 -- is =0.6960352422907489\n",
      "accuracy for K = 15 -- is =0.7092511013215859\n",
      "accuracy for K = 16 -- is =0.7180616740088106\n",
      "accuracy for K = 17 -- is =0.7180616740088106\n",
      "accuracy for K = 18 -- is =0.7180616740088106\n",
      "accuracy for K = 19 -- is =0.7312775330396476\n",
      "accuracy for K = 20 -- is =0.7533039647577092\n",
      "accuracy for K = 21 -- is =0.7577092511013216\n",
      "accuracy for K = 22 -- is =0.7665198237885462\n",
      "accuracy for K = 23 -- is =0.7665198237885462\n",
      "accuracy for K = 24 -- is =0.7709251101321586\n",
      "accuracy for K = 25 -- is =0.7797356828193832\n",
      "accuracy for K = 26 -- is =0.788546255506608\n",
      "accuracy for K = 27 -- is =0.801762114537445\n",
      "accuracy for K = 28 -- is =0.801762114537445\n",
      "accuracy for K = 29 -- is =0.801762114537445\n",
      "accuracy for K = 30 -- is =0.8061674008810573\n",
      "accuracy for K = 31 -- is =0.8105726872246696\n",
      "accuracy for K = 32 -- is =0.8105726872246696\n",
      "accuracy for K = 33 -- is =0.8237885462555066\n",
      "accuracy for K = 34 -- is =0.8281938325991189\n",
      "accuracy for K = 35 -- is =0.8325991189427313\n",
      "accuracy for K = 36 -- is =0.8370044052863436\n",
      "accuracy for K = 37 -- is =0.8370044052863436\n",
      "accuracy for K = 38 -- is =0.8414096916299559\n",
      "accuracy for K = 39 -- is =0.8414096916299559\n",
      "accuracy for K = 40 -- is =0.8414096916299559\n",
      "accuracy for K = 41 -- is =0.8458149779735683\n",
      "accuracy for K = 42 -- is =0.8458149779735683\n",
      "accuracy for K = 43 -- is =0.8502202643171806\n",
      "accuracy for K = 44 -- is =0.8502202643171806\n",
      "accuracy for K = 45 -- is =0.8502202643171806\n",
      "accuracy for K = 46 -- is =0.8502202643171806\n",
      "accuracy for K = 47 -- is =0.8502202643171806\n",
      "accuracy for K = 48 -- is =0.8590308370044053\n",
      "accuracy for K = 49 -- is =0.8678414096916299\n",
      "accuracy for K = 50 -- is =0.8678414096916299\n",
      "accuracy for K = 51 -- is =0.8678414096916299\n",
      "accuracy for K = 52 -- is =0.8678414096916299\n",
      "accuracy for K = 53 -- is =0.8678414096916299\n",
      "accuracy for K = 54 -- is =0.8766519823788547\n",
      "accuracy for K = 55 -- is =0.8810572687224669\n",
      "accuracy for K = 56 -- is =0.8810572687224669\n",
      "accuracy for K = 57 -- is =0.8810572687224669\n",
      "accuracy for K = 58 -- is =0.8854625550660793\n",
      "accuracy for K = 59 -- is =0.8854625550660793\n",
      "accuracy for K = 60 -- is =0.8854625550660793\n",
      "accuracy for K = 61 -- is =0.8854625550660793\n",
      "accuracy for K = 62 -- is =0.8898678414096917\n",
      "accuracy for K = 63 -- is =0.8898678414096917\n",
      "accuracy for K = 64 -- is =0.8898678414096917\n",
      "accuracy for K = 65 -- is =0.8986784140969163\n",
      "accuracy for K = 66 -- is =0.8986784140969163\n",
      "accuracy for K = 67 -- is =0.8986784140969163\n",
      "accuracy for K = 68 -- is =0.8986784140969163\n",
      "accuracy for K = 69 -- is =0.9074889867841409\n",
      "accuracy for K = 70 -- is =0.9074889867841409\n",
      "accuracy for K = 71 -- is =0.9074889867841409\n",
      "accuracy for K = 72 -- is =0.9118942731277533\n",
      "accuracy for K = 73 -- is =0.9118942731277533\n",
      "accuracy for K = 74 -- is =0.9162995594713657\n",
      "accuracy for K = 75 -- is =0.920704845814978\n",
      "accuracy for K = 76 -- is =0.920704845814978\n",
      "accuracy for K = 77 -- is =0.920704845814978\n",
      "accuracy for K = 78 -- is =0.920704845814978\n",
      "accuracy for K = 79 -- is =0.920704845814978\n",
      "accuracy for K = 80 -- is =0.9251101321585903\n",
      "accuracy for K = 81 -- is =0.9251101321585903\n",
      "accuracy for K = 82 -- is =0.9295154185022027\n",
      "accuracy for K = 83 -- is =0.9295154185022027\n",
      "accuracy for K = 84 -- is =0.933920704845815\n",
      "accuracy for K = 85 -- is =0.9383259911894273\n",
      "accuracy for K = 86 -- is =0.9427312775330396\n",
      "accuracy for K = 87 -- is =0.9427312775330396\n",
      "accuracy for K = 88 -- is =0.947136563876652\n",
      "accuracy for K = 89 -- is =0.947136563876652\n",
      "accuracy for K = 90 -- is =0.947136563876652\n",
      "accuracy for K = 91 -- is =0.947136563876652\n",
      "accuracy for K = 92 -- is =0.9515418502202643\n",
      "accuracy for K = 93 -- is =0.9559471365638766\n",
      "accuracy for K = 94 -- is =0.9559471365638766\n",
      "accuracy for K = 95 -- is =0.9559471365638766\n",
      "accuracy for K = 96 -- is =0.9559471365638766\n",
      "accuracy for K = 97 -- is =0.9559471365638766\n",
      "accuracy for K = 98 -- is =0.960352422907489\n",
      "accuracy for K = 99 -- is =0.960352422907489\n",
      "accuracy for K = 100 -- is =0.960352422907489\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "def get_DPR_acc(questions, K_Values):\n",
    "    maxK = max(K_Values)\n",
    "    cont = 0\n",
    "    ids = []\n",
    "    for line in questions:\n",
    "        ids.append([])\n",
    "        result = pipeline.run(query=line[QUESTION_COLUMN],params={\"Retriever\": {\"top_k\": maxK}})\n",
    "        document_dict = ast.literal_eval(str(result[\"documents\"]).replace(\"<Document: \",\"\").replace(\"'}>\",\"'}\"))\n",
    "        for i in range(len(document_dict)):\n",
    "            ids[cont].append(int(document_dict[i][\"meta\"][\"idarticle\"]))\n",
    "        cont+=1\n",
    "    corrects = []\n",
    "    accuracies = []\n",
    "    for j in range(len(K_Values)):\n",
    "        corrects.append(0)\n",
    "        cont = 0\n",
    "        for line in questions:\n",
    "            if int(line[len(line)-1]) in ids[cont][:K_Values[j]]:\n",
    "                corrects[j]+=1\n",
    "            cont+=1\n",
    "        accuracies.append(corrects[j]/len(questions))\n",
    "    return(accuracies)\n",
    "\n",
    "accs = []\n",
    "#    Ks = [1, 2, 5, 10, 25, 50, 100]\n",
    "Ks = range(1,101)\n",
    "accs = get_DPR_acc( pira_test, Ks)\n",
    "for i in range(len(Ks)):\n",
    "    print(\"accuracy for K = \" + str(Ks[i]) + \" -- is =\" + str(accs[i]))\n",
    "    \n",
    "df_accs = pd.DataFrame(accs)\n",
    "df_accs.to_csv(\"DPR_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Haystack data prep bm25.ipynb",
   "provenance": [
    {
     "file_id": "10r7O6VkA5Ej6RBgeboFFvGrS7MdEMMFv",
     "timestamp": 1621628528806
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
