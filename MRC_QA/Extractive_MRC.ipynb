{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cdb3cb",
   "metadata": {},
   "source": [
    "# Extractive Approach for Machine Reading Comprehension (MRC)\n",
    "\n",
    "This Jupyter notebook evaluates the performance of extractive Question-Answering transformers models on Pirá Dataset. \n",
    "\n",
    "Extractive Models generate the answer as spam of the supporting text.\n",
    "\n",
    "Check the full GitHub at: https://github.com/C4AI/Pira"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00b3db",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11682f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import pandas as pd\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97d031",
   "metadata": {},
   "source": [
    "## Dataset information\n",
    "\n",
    "Here we set some values necessary to load the dataset\n",
    "\n",
    "PATH_BASE -> Dataset path\n",
    "\n",
    "\n",
    "SUPPORTING_TEXT_COLUMN -> Indicates the Supporting Text Column. Use \"10\" for English or \"-2\" for Portuguese.\n",
    "\n",
    "ANSWER_COLUMN -> Indicates the Answer Column. Use \"6\" for English or \"7\" for Portuguese.\n",
    "\n",
    "QUESTION_COLUMN -> Indicates the Answer Column. Use \"2\" for English or \"3\" for Portuguese.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61485997",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BASE = './Data/test.csv'\n",
    "\n",
    "SUPPORTING_TEXT_COLUMN = -2\n",
    "ANSWER_COLUMN = 7\n",
    "QUESTION_COLUMN = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293d696",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ad347a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pira_dataset = pd.read_csv(PATH_BASE).values.tolist()\n",
    "    \n",
    "quest = []\n",
    "for line in pira_dataset:\n",
    "    quest.append([str(line[QUESTION_COLUMN]), str(line[ANSWER_COLUMN]), str(line[SUPPORTING_TEXT_COLUMN])])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3001049e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O que permitiu a descoberta de novas reservas de petróleo e gás distantes da costa nos últimos 10 anos?',\n",
       " 'Avanço tecnológico',\n",
       " 'Os adiantamentos no conhecimento e capacidade nova exploração e desenvolvimento em áreas offshore continuam sendo uma importante fonte de aumento da produção global de petróleo e gás. Os avanços tecnológicos na última década incentivaram a exploração nas águas profundas e ultradeeste a mais longe da costa e permitiram a descoberta de novas reservas significativas. As capacidades de profundidade da água para a exploração offshore aumentaram de cerca de 3.050 m a mais de 3.350 m entre 2010 e 2018, enquanto a capacidade de produção utilizando plataformas flutuantes atingiu quase 2.900 m em 2018, de 2.438 m em 2010 (Barton e outros, 2019). Tais avanços tecnológicos permitiram em parte a expansão do setor offshore de petróleo e gás para novas regiões, incluindo o Mediterrâneo oriental e as áreas da costa da Guiana. Também houve avanços na compreensão dos potenciais impactos ambientais e sociais das atividades de exploração e produção no ambiente circundante e no desenvolvimento de novas abordagens para mitigar os impactos. Por exemplo, o Reino Unido da Grã-Bretanha e da Irlanda do Norte criaram um registro de ruído marinho para registrar atividades humanas que produzem ruído impulsivo alto (10 Hz- 10 kHz) nos mares em torno de seu território. Essa iniciativa pretende criar dados de linha de base e quantificar a pressão sobre o meio ambiente de atividades antropogênicas associadas à exploração e desenvolvimento de hidrocarbonetos, incluindo pesquisas sísmicas, perfil sub-inferiores e condução de pilha. Da mesma forma, o projeto da serpente, que significa \"ROV científico e ambiental (veículo operado remotamente) usando a tecnologia industrial existente\", é um exemplo de colaboração internacional entre a comunidade científica, os reguladores ambientais e a indústria de petróleo e gás para reunir e fornecer linha de base Informações sobre ecossistemas em torno de instalações de petróleo e gás offshore usando veículos operados remotamente de ponta que podem operar no Oceano Profundo (Projeto Serpente, 2020). Mais recentemente, a indústria de petróleo e gás offshore contribuiu para o setor MRE, fornecendo especialização para a construção, manutenção e descomissionamento de projetos de vento offshore em escala de serviços públicos. Os conceitos de design e engenharia estrutural para as turbinas eólicas flutuantes, que podem expandir significativamente o desenvolvimento de energia eólica em águas mais profundas associadas a recursos eólicos mais altos, são amplamente influenciados por instalações de óleo e gás profundas (agência internacional de energia renovável, 2016).']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quest[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79c94f",
   "metadata": {},
   "source": [
    "## Iniatializing the model\n",
    "\n",
    "Initializing the Extractive QA model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71723a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be1aff3554c48ff8be9f651e8792c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/862 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39112c6aa8b140b096bc92b8108e03fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f4929a5741496ebdf71276a80712c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/494 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c4a632791d450c928c6a76240eb78d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/210k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367f4dd317704b2685a4df3a5c85ec03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5075850486755371, 'start': 23, 'end': 39, 'answer': 'February 7, 2016'}\n"
     ]
    }
   ],
   "source": [
    "model_name = \"pierreguillou/bert-base-cased-squad-v1.1-portuguese\"\n",
    "\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model = model_name,\n",
    "    tokenizer = model_name\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "predictions = qa_pipeline({\n",
    "    'context': \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n",
    "    'question': \"What day was the game played on?\"\n",
    "})\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff2812f",
   "metadata": {},
   "source": [
    "## Generating each answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a2b528c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "true_answers = []\n",
    "gen_answers = []\n",
    "passages = []\n",
    "questions =[]\n",
    "for i in range(len(quest)):\n",
    "    print(i)\n",
    "    predictions = qa_pipeline({\n",
    "    'context': quest[i][2],\n",
    "    'question': quest[i][0]\n",
    "    })\n",
    "    passages.append(quest[i][2])\n",
    "    questions.append(quest[i][0])\n",
    "    gen_answers.append(str(predictions[\"answer\"]))\n",
    "    true_answers.append([quest[i][1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d69c9",
   "metadata": {},
   "source": [
    "## Evaluationg script\n",
    "\n",
    "SQuAD evaluation script: https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py \n",
    "\n",
    "Modified slightly for this notebook since we do not remove articles to remain consistent for both Portuguese and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410cdcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation and extra whitespace.\"\"\"\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punc(lower(s)))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(gold_answers, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "\n",
    "    for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "      total += 1\n",
    "      exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "      f1 += metric_max_over_ground_truths(\n",
    "          f1_score, prediction, ground_truths)\n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7608d02",
   "metadata": {},
   "source": [
    "## Performing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3d4357c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 4.405286343612334, 'f1': 37.531883499754755}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(true_answers, gen_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
