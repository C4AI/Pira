{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82393af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# PARAMETERS\n",
    "openai.api_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "\n",
    "MODELS = ['gpt-3.5-turbo', 'gpt-4']\n",
    "\n",
    "DATASET = 'test_sets/test_qa.csv'\n",
    "\n",
    "PROMPT_EN = \"Answer in AS FEW WORDS AS POSSIBLE. ---\\n\\nQuestion: {}{}\\n\\nAnswer: \"\n",
    "PROMPT_PT = \"Responda com o MÍNIMO DE PALAVRAS POSSÍVEL. ---\\n\\nPergunta: {}{}\\n\\nResposta: \"\n",
    "\n",
    "PROMPT_EN_CONTEXT = \"Answer the question in AS FEW WORDS AS POSSIBLE and based on the context below.\\n\\nContext: {}\\n\\n\\---\\n\\nQuestion: {}\\nAnswer: \"\n",
    "PROMPT_PT_CONTEXT = \"Responda à pergunta com o MÍNIMO DE PALAVRAS POSSÍVEL e com base no contexto abaixo.\\n\\nContexto: {}\\n\\n\\---\\n\\nPergunta: {}\\nResposta: \"\n",
    "\n",
    "USECOLS = ['id_qa', 'corpus', \n",
    "              'abstract', 'abstract_translated_pt',\n",
    "              'question_en_origin', 'answer_en_origin', \n",
    "              'question_pt_origin', 'answer_pt_origin']\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    # 1) Pergunta em E, s/ texto\n",
    "    '1_Qen': {\n",
    "        'prompt': PROMPT_EN,\n",
    "        'question': 'question_en_origin',\n",
    "        'context': '',\n",
    "        'answer': 'answer_en_origin'\n",
    "    },\n",
    "    # 2) Pergunta em P, s/ texto\n",
    "    '2_Qpt': {\n",
    "        'prompt': PROMPT_PT,\n",
    "        'question': 'question_pt_origin',\n",
    "        'context': '',\n",
    "        'answer': 'answer_pt_origin'\n",
    "    },\n",
    "    # 3) Pergunta em E, texto em E\n",
    "    '3_Qen_Cen': {\n",
    "        'prompt': PROMPT_EN_CONTEXT,\n",
    "        'question': 'question_en_origin',\n",
    "        'context': 'abstract',\n",
    "        'answer': 'answer_en_origin',\n",
    "    },\n",
    "    # 4) Pergunta em P, texto em P\n",
    "    '4_Qpt_Cpt': {\n",
    "        'prompt': PROMPT_PT_CONTEXT,\n",
    "        'question': 'question_pt_origin',\n",
    "        'context': 'abstract_translated_pt',\n",
    "        'answer': 'answer_pt_origin',\n",
    "    },\n",
    "    # 5) Pergunta em P, texto em E\n",
    "    '5_Qpt_Cen': {\n",
    "        'prompt': PROMPT_PT_CONTEXT,\n",
    "        'question': 'question_pt_origin',\n",
    "        'context': 'abstract',\n",
    "        'answer': 'answer_pt_origin'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation and extra whitespace.\"\"\"\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punc(lower(s)))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "# def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "#     scores_for_ground_truths = []\n",
    "#     for ground_truth in ground_truths:\n",
    "#         score = metric_fn(prediction, ground_truth)\n",
    "#         scores_for_ground_truths.append(score)\n",
    "#     return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "# def evaluate(gold_answers, predictions):\n",
    "#     f1 = exact_match = total = 0\n",
    "\n",
    "#     for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "#       total += 1\n",
    "#       exact_match += metric_max_over_ground_truths(\n",
    "#                     exact_match_score, prediction, ground_truths)\n",
    "#       f1 += metric_max_over_ground_truths(\n",
    "#           f1_score, prediction, ground_truths)\n",
    "    \n",
    "#     exact_match = 100.0 * exact_match / total\n",
    "#     f1 = 100.0 * f1 / total\n",
    "\n",
    "#     return {'exact_match': exact_match, 'f1': f1}\n",
    "\n",
    "\n",
    "def chatgpt_answer(prompt, question, context='', model='gpt-3.5-turbo'):\n",
    "    # Available models: 'gpt-3.5-turbo' and 'gpt-4'\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        temperature=0.0,\n",
    "        max_tokens=100,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        messages=[\n",
    "            {'role': 'system', \n",
    "             'content': prompt.format(context, question)}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATASET, usecols=USECOLS)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in MODELS:\n",
    "    print(f'\\n\\n>>> MODEL: {model}')\n",
    "\n",
    "    for exp in EXPERIMENTS:\n",
    "        print(f'>>>>>> Experiment: {exp}')\n",
    "        \n",
    "        prompt = EXPERIMENTS[exp]['prompt']\n",
    "        question_col = EXPERIMENTS[exp]['question']\n",
    "        context_col = EXPERIMENTS[exp]['context']\n",
    "        answer_col = EXPERIMENTS[exp]['answer']\n",
    "\n",
    "        gpt = []\n",
    "        f1 = []\n",
    "        em = []\n",
    "        for _, row in tqdm(df.iterrows()):\n",
    "            question = row[question_col]\n",
    "            context = None if question_col == '' else row[question_col]\n",
    "\n",
    "            ans = chatgpt_answer(prompt, question, context, model)\n",
    "            \n",
    "            gpt.append(ans)\n",
    "            f1.append(f1_score(ans, row[answer_col]))\n",
    "            em.append(exact_match_score(ans, row[answer_col]))\n",
    "\n",
    "        df['gpt_answers'] = gpt\n",
    "        df['gpt_f1'] = f1\n",
    "        df['gpt_em'] = em\n",
    "\n",
    "        df.to_csv(f'results/experiments_chatgpt_qa/{model}_{exp}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
