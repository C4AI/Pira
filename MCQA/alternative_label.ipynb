{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "datasets = ['MCQA-train.csv', 'MCQA-validation.csv', 'MCQA-test.csv']\n",
        "\n",
        "for dataset in datasets:\n",
        "\n",
        "  df = pd.read_csv(dataset)\n",
        "\n",
        "  alternatives = df[['A', 'B', 'C', 'D', 'E']]\n",
        "\n",
        "  # Create a list of lists\n",
        "  list_of_lists = alternatives.values.tolist()\n",
        "\n",
        "  # Shuffle the elements in each sublist\n",
        "  for sublist in list_of_lists:\n",
        "      random.shuffle(sublist)\n",
        "\n",
        "  # Create a dataset with the shuffled alternatives\n",
        "  df_shuffled = pd.DataFrame(list_of_lists)\n",
        "\n",
        "  # Replace them in the original dataset\n",
        "  df.loc[:, ['A', 'B', 'C', 'D', 'E']] = df_shuffled.values\n",
        "\n",
        "  # Remove wrong column\n",
        "  df = df.drop('answer_alternative', axis=1)\n",
        "\n",
        "  # Clean with regex\n",
        "  df = df.replace(r'\\r+|\\n+|\\t+','', regex=True)\n",
        "\n",
        "  # F1 score\n",
        "  def normalize_answer(s):\n",
        "\n",
        "    def white_space_fix(text):\n",
        "      return ' '.join(text.split())\n",
        "      \n",
        "    def remove_punc(text):\n",
        "      exclude = set(string.punctuation)\n",
        "      return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "      return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(s)))\n",
        "\n",
        "  def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "      return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall) * 100\n",
        "    return f1\n",
        "\n",
        "  # Find alternative \n",
        "  df['value_A'] = df.apply(lambda row: f1_score(row['A'], row['correct']), axis=1)\n",
        "  df['value_B'] = df.apply(lambda row: f1_score(row['B'], row['correct']), axis=1)\n",
        "  df['value_C'] = df.apply(lambda row: f1_score(row['C'], row['correct']), axis=1)\n",
        "  df['value_D'] = df.apply(lambda row: f1_score(row['D'], row['correct']), axis=1)\n",
        "  df['value_E'] = df.apply(lambda row: f1_score(row['E'], row['correct']), axis=1)\n",
        "\n",
        "  # Select a subset of columns to compare\n",
        "  cols_to_compare = ['value_A', 'value_B', 'value_C', 'value_D', 'value_E']\n",
        "\n",
        "  # Find the column name with the largest value for each row in the subset of columns\n",
        "  df['alternative'] = df[cols_to_compare].idxmax(axis=1).str[-1]\n",
        "\n",
        "  df = df.drop(['value_A', 'value_B', 'value_C', 'value_D', 'value_E'], axis=1)\n",
        "\n",
        "  df.to_csv(dataset)"
      ],
      "metadata": {
        "id": "uVW22jR03KSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}