{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cdb3cb",
   "metadata": {},
   "source": [
    "# BM25 Retriver for Open Question Answering for Pira\n",
    "\n",
    "This Jupyter notebook evaluates the performance of BM25 retriever model on Pirá Dataset. \n",
    "\n",
    "The code is based on DPR Haystack library implementation: https://haystack.deepset.ai/overview/intro\n",
    "\n",
    "Check the full Pira GitHub at: https://github.com/C4AI/Pira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from haystack.utils import launch_es\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing tasks\n",
    "\n",
    "DO_PREPROCESSING -> If set to True, the supporting texts are split into 100 words chunks.\n",
    "\n",
    "PUSH_DOCUMENTS -> Push documents to ElasticSearch. It is only needed the first time.\n",
    "\n",
    "SAVE_QUESTIONS_WITH_PASSAGES -> Save the questions with the passages retrieved by BM25.\n",
    "\n",
    "DO_EVAL -> Performs a evaluation of the Retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_PREPROCESSING = True\n",
    "PUSH_DOCUMENTS = True\n",
    "SAVE_QUESTIONS_WITH_PASSAGES = True\n",
    "DO_EVAL = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xNT45Lr7mi9h"
   },
   "outputs": [],
   "source": [
    "\n",
    "PATH_BASE = 'splitted_data/'\n",
    "ABSTRACT_COLUMN = 18\n",
    "ANSWER_COLUMN = 7\n",
    "QUESTION_COLUMN = 3\n",
    "\n",
    "INDEX_KNOWLEDGE_BASE = \"abstracts_100_pt\"\n",
    "NUMBER_OF_WORDS = 100\n",
    "NUMBER_OF_PASSAGES = 5\n",
    "\n",
    "PATH_SAVE_BM25_EVAL = \"Retriever_Results/BM25_results_Abstract_Translated.csv\"\n",
    "\n",
    "PATH_SAVE_QUESTIONS_BM25 = 'finetune_PT_PT_100Words_5Passages/' \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "It is important to ensure that we do not use the same supporting text more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pira_train = pd.read_csv(PATH_BASE + \"train.csv\").values.tolist()\n",
    "pira_val = pd.read_csv(PATH_BASE + \"validation.csv\").values.tolist()\n",
    "pira_test = pd.read_csv(PATH_BASE + \"test.csv\").values.tolist()\n",
    "\n",
    "pira_dataset = pira_train + pira_val + pira_test\n",
    "\n",
    "abstracts = []\n",
    "temp = []\n",
    "for i in range(len(pira_dataset)):\n",
    "    if pira_dataset[i][ABSTRACT_COLUMN] not in temp:\n",
    "        abstracts.append([pira_dataset[i][ABSTRACT_COLUMN], len(abstracts)+1])\n",
    "        temp.append(pira_dataset[i][ABSTRACT_COLUMN])\n",
    "del temp \n",
    " \n",
    "for i in range(len(pira_dataset)):\n",
    "    for j in range(len(abstracts)):\n",
    "        if pira_dataset[i][ABSTRACT_COLUMN] == abstracts[j][0]:\n",
    "            pira_dataset[i].append(abstracts[j][1])\n",
    "            \n",
    "dicts = []\n",
    "for line in abstracts:\n",
    "    dicts.append({'content' : line[0], 'meta' : {'idarticle': line[1]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opDV9WHuD5u1",
    "outputId": "bce7fb56-109a-4a78-c0d7-e9742509407c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DO_PREPROCESSING:\n",
    "\n",
    "    processor = PreProcessor(\n",
    "        clean_empty_lines=True,\n",
    "        clean_whitespace=True,\n",
    "        clean_header_footer=True,\n",
    "        split_by=\"word\",\n",
    "        split_length=NUMBER_OF_WORDS,\n",
    "        split_respect_sentence_boundary=True,\n",
    "        split_overlap=0\n",
    "    )\n",
    "    docs = []\n",
    "    cont=0\n",
    "\n",
    "    for dict1 in dicts:\n",
    "        if cont %100 ==0:\n",
    "            print(cont)\n",
    "        cont+=1\n",
    "        doc = processor.process(dict1)\n",
    "        docs = docs+doc\n",
    "else:\n",
    "    docs = dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing ElasticSearch\n",
    "\n",
    "To Download ElasticSearch files, uncoment top lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGrpGy0KDhDb"
   },
   "outputs": [],
   "source": [
    "#! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
    "#! tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
    "#! sudo chown -R daemon:daemon elasticsearch-7.9.2\n",
    "\n",
    "launch_es()\n",
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "\n",
    "# wait until ES has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the document store and writing supporting documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1621624557643,
     "user": {
      "displayName": "Marcos Menon Jose",
      "photoUrl": "",
      "userId": "06041167050429695478"
     },
     "user_tz": 180
    },
    "id": "_MiLIWxiDj19",
    "outputId": "de8890d9-6825-4f05-ef65-b4ab39bcb68a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=INDEX_KNOWLEDGE_BASE)\n",
    "if PUSH_DOCUMENTS:\n",
    "    document_store.write_documents(docs,batch_size=1000)\n",
    "#document_store.delete_all_documents # Deleting documents if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Retriever Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ElasticsearchRetriever(document_store=document_store)\n",
    "\n",
    "pipeline = DocumentSearchPipeline(retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"O que é o Pré-Sal ?\"\n",
    "\n",
    "\n",
    "result = pipeline.run(\n",
    "    query=question,\n",
    "    params={\n",
    "        \"Retriever\": {\n",
    "            \"top_k\": 5,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating BM25 performance for multiple k values\n",
    "\n",
    "This function checks for each question if the supporting text was one of the top k retrieved documents and generates the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DO_EVAL:\n",
    "    import ast\n",
    "\n",
    "    def get_BM25_acc(questions, K_Values):\n",
    "        maxK = max(K_Values)\n",
    "        cont = 0\n",
    "        ids = []\n",
    "        for line in questions:\n",
    "            ids.append([])\n",
    "            result = pipeline.run(query=line[QUESTION_COLUMN],params={\"Retriever\": {\"top_k\": maxK}})\n",
    "            document_dict = ast.literal_eval(str(result[\"documents\"]).replace(\"<Document: \",\"\").replace(\"'}>\",\"'}\"))\n",
    "            for i in range(len(document_dict)):\n",
    "                ids[cont].append(int(document_dict[i][\"meta\"][\"idarticle\"]))\n",
    "            cont+=1\n",
    "        corrects = []\n",
    "        accuracies = []\n",
    "        for j in range(len(K_Values)):\n",
    "            corrects.append(0)\n",
    "            cont = 0\n",
    "            for line in questions:\n",
    "                if int(line[len(line)-1]) in ids[cont][:K_Values[j]]:\n",
    "                    corrects[j]+=1\n",
    "                cont+=1\n",
    "            accuracies.append(corrects[j]/len(questions))\n",
    "        return(accuracies)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    Ks = range(1,101)\n",
    "    pira_test2 = pd.DataFrame(pira_test)\n",
    "    test = pira_test2.dropna(subset=[pira_test2.columns[QUESTION_COLUMN]]).values.tolist()\n",
    "    accs = get_BM25_acc( test, Ks)\n",
    "    for i in range(len(Ks)):\n",
    "        print(\"accuracy for K = \" + str(Ks[i]) + \" -- is =\" + str(accs[i]))\n",
    "    \n",
    "    df_accs = pd.DataFrame(accs)\n",
    "    df_accs.to_csv(PATH_SAVE_BM25_EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Questions with Retrieved Passages for the Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if SAVE_QUESTIONS_WITH_PASSAGES:\n",
    "    isExist = os.path.exists(PATH_SAVE_QUESTIONS_BM25)\n",
    "\n",
    "    if not isExist:\n",
    "        os.makedirs(PATH_SAVE_QUESTIONS_BM25)\n",
    "\n",
    "    train_docs = [] \n",
    "    for i in range(len(pira_train)):\n",
    "        result = pipeline.run(query=pira_train[i][QUESTION_COLUMN],params={\"Retriever\": {\"top_k\": NUMBER_OF_PASSAGES}})\n",
    "        document_dict = ast.literal_eval(str(result[\"documents\"]).replace(\"<Document: \",\"\").replace(\"'}>\",\"'}\"))\n",
    "        question = pira_train[i][QUESTION_COLUMN] + \"  context:\"\n",
    "        for j in range(len(document_dict)):\n",
    "            question += \" \" + document_dict[j][\"content\"]\n",
    "        train_docs.append([question, pira_train[i][ANSWER_COLUMN]])\n",
    "    train_df = pd.DataFrame(train_docs)\n",
    "    train_df.to_csv(PATH_SAVE_QUESTIONS_BM25 + \"train.csv\")\n",
    "    \n",
    "\n",
    "    \n",
    "    val_docs = [] \n",
    "    for i in range(len(pira_val)):\n",
    "        result = pipeline.run(query=pira_val[i][QUESTION_COLUMN],params={\"Retriever\": {\"top_k\": NUMBER_OF_PASSAGES}})\n",
    "        document_dict = ast.literal_eval(str(result[\"documents\"]).replace(\"<Document: \",\"\").replace(\"'}>\",\"'}\"))\n",
    "        question = pira_val[i][QUESTION_COLUMN] + \"  context:\"\n",
    "        for j in range(len(document_dict)):\n",
    "            question += \" \" + document_dict[j][\"content\"]\n",
    "        val_docs.append([question, pira_val[i][ANSWER_COLUMN]])\n",
    "    val_df = pd.DataFrame(val_docs)\n",
    "    val_df.to_csv(PATH_SAVE_QUESTIONS_BM25 + \"val.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    test_docs = [] \n",
    "    for i in range(len(pira_test)):\n",
    "        result = pipeline.run(query=pira_test[i][QUESTION_COLUMN],params={\"Retriever\": {\"top_k\": NUMBER_OF_PASSAGES}})\n",
    "        document_dict = ast.literal_eval(str(result[\"documents\"]).replace(\"<Document: \",\"\").replace(\"'}>\",\"'}\"))\n",
    "        question = pira_test[i][QUESTION_COLUMN] + \"  context:\"\n",
    "        for j in range(len(document_dict)):\n",
    "            question += \" \" + document_dict[j][\"content\"]\n",
    "        test_docs.append([question, pira_test[i][ANSWER_COLUMN]])\n",
    "    test_df = pd.DataFrame(test_docs)\n",
    "    test_df.to_csv(PATH_SAVE_QUESTIONS_BM25 + \"test.csv\")\n",
    "\n",
    "\n",
    "    extractive_docs = [] \n",
    "    for i in range(len(pira_test)):\n",
    "        result = pipeline.run(query = pira_test[i][QUESTION_COLUMN],params={\"Retriever\": {\"top_k\": NUMBER_OF_PASSAGES}})\n",
    "        document_dict = ast.literal_eval(str(result[\"documents\"]).replace(\"<Document: \",\"\").replace(\"'}>\",\"'}\"))\n",
    "        context = document_dict[0][\"content\"]\n",
    "        for j in range(1, len(document_dict)):\n",
    "            context += \" \" + document_dict[j][\"content\"]\n",
    "        extractive_docs.append([pira_test[i][QUESTION_COLUMN], pira_test[i][ANSWER_COLUMN], context])\n",
    "    extractive_df = pd.DataFrame(extractive_docs)\n",
    "    extractive_df.to_csv(PATH_SAVE_QUESTIONS_BM25 + \"extractive.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pira_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pira_test[0][ANSWER_COLUMN]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Haystack data prep bm25.ipynb",
   "provenance": [
    {
     "file_id": "10r7O6VkA5Ej6RBgeboFFvGrS7MdEMMFv",
     "timestamp": 1621628528806
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
