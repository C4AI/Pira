{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5518bc70",
   "metadata": {},
   "source": [
    "# Extractive Approach for Open Question Answering\n",
    "\n",
    "This Jupyter notebook evaluates the performance of extractive Question-Answering transformers models on Pir√° Dataset after the retriever step. \n",
    "\n",
    "Extractive Models generate the answer as spam of the supporting text.\n",
    "\n",
    "Check the full GitHub at: https://github.com/C4AI/Pira"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22defcaf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34230cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935858e",
   "metadata": {},
   "source": [
    "## Dataset information\n",
    "\n",
    "Be sure to run the BM25.ipynb file before to save retrieved supporting texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13db0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BASE = 'finetune_PT_PT_100Words_5Passages/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293d696",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a44644",
   "metadata": {},
   "outputs": [],
   "source": [
    "pira_dataset = pd.read_csv(PATH_BASE + \"extractive.csv\", index_col = 0).values.tolist()\n",
    "    \n",
    "quest = []\n",
    "for line in pira_dataset:\n",
    "    quest.append([str(line[0]), str(line[1]), str(line[2])])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db79c94f",
   "metadata": {},
   "source": [
    "## Iniatializing the model\n",
    "\n",
    "Initializing the Extractive QA model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f974ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.5075850486755371, 'start': 23, 'end': 39, 'answer': 'February 7, 2016'}\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\",\n",
    "    tokenizer=\"pierreguillou/bert-base-cased-squad-v1.1-portuguese\"\n",
    ")\n",
    "\n",
    "predictions = qa_pipeline({\n",
    "    'context': \"The game was played on February 7, 2016 at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n",
    "    'question': \"What day was the game played on?\"\n",
    "})\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff2812f",
   "metadata": {},
   "source": [
    "## Generating each answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447fff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_answers = []\n",
    "gen_answers = []\n",
    "passages = []\n",
    "questions =[]\n",
    "for i in range(len(quest)):\n",
    "    print(i)\n",
    "    predictions = qa_pipeline({\n",
    "    'context': quest[i][2],\n",
    "    'question': quest[i][0]\n",
    "    })\n",
    "    passages.append(quest[i][2])\n",
    "    questions.append(quest[i][0])\n",
    "    gen_answers.append(str(predictions[\"answer\"]))\n",
    "    true_answers.append([quest[i][1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d69c9",
   "metadata": {},
   "source": [
    "## Evaluationg script\n",
    "\n",
    "SQuAD evaluation script: https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py \n",
    "\n",
    "Modified slightly for this notebook since we do not remove articles to remain consistent for both Portuguese and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation and extra whitespace.\"\"\"\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punc(lower(s)))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def evaluate(gold_answers, predictions):\n",
    "    f1 = exact_match = total = 0\n",
    "\n",
    "    for ground_truths, prediction in zip(gold_answers, predictions):\n",
    "      total += 1\n",
    "      exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "      f1 += metric_max_over_ground_truths(\n",
    "          f1_score, prediction, ground_truths)\n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7608d02",
   "metadata": {},
   "source": [
    "## Performing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc08abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(true_answers, gen_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
