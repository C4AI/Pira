{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2745b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa275e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "language = 'english'\n",
    "question_type = ['standard', 'human_paraphrase', 'automatic_paraphrase']\n",
    "threshold = 3\n",
    "dataset_types = ['soft', 'rigid', 'discrete']\n",
    "model_type = 'distilbert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438cd55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: standard soft\n",
      "Simple assignment\n",
      "Accuracy: 0.8287037037037037\n",
      "F1-score: 0.45316455696202534\n",
      "Naive Bayes\n",
      "Accuracy: 0.8287037037037037\n",
      "F1-score: 0.45316455696202534\n",
      "Test: standard rigid\n",
      "Simple assignment\n",
      "Accuracy: 0.9379310344827586\n",
      "F1-score: 0.48398576512455516\n",
      "Naive Bayes\n",
      "Accuracy: 0.9379310344827586\n",
      "F1-score: 0.48398576512455516\n",
      "Test: standard discrete\n",
      "Simple assignment\n",
      "Accuracy: 0.8287037037037037\n",
      "F1-score: 0.45316455696202534\n",
      "Naive Bayes\n",
      "Accuracy: 0.8287037037037037\n",
      "F1-score: 0.45316455696202534\n",
      "Test: human_paraphrase soft\n",
      "Simple assignment\n",
      "Accuracy: 0.8287037037037037\n",
      "F1-score: 0.45316455696202534\n",
      "Naive Bayes\n",
      "Accuracy: 0.7824074074074074\n",
      "F1-score: 0.438961038961039\n",
      "Test: human_paraphrase rigid\n",
      "Simple assignment\n",
      "Accuracy: 0.9379310344827586\n",
      "F1-score: 0.48398576512455516\n",
      "Naive Bayes\n",
      "Accuracy: 0.9379310344827586\n",
      "F1-score: 0.48398576512455516\n",
      "Test: human_paraphrase discrete\n",
      "Simple assignment\n",
      "Accuracy: 0.8287037037037037\n",
      "F1-score: 0.45316455696202534\n",
      "Naive Bayes\n",
      "Accuracy: 0.7824074074074074\n",
      "F1-score: 0.438961038961039\n",
      "Test: automatic_paraphrase soft\n",
      "Simple assignment\n",
      "Accuracy: 0.8287037037037037\n",
      "F1-score: 0.45316455696202534\n",
      "Naive Bayes\n",
      "Accuracy: 0.6805555555555556\n",
      "F1-score: 0.4433613445378151\n",
      "Test: automatic_paraphrase rigid\n",
      "Simple assignment\n",
      "Accuracy: 0.9379310344827586\n",
      "F1-score: 0.48398576512455516\n",
      "Naive Bayes\n",
      "Accuracy: 0.9241379310344827\n",
      "F1-score: 0.4802867383512545\n",
      "Test: automatic_paraphrase discrete\n",
      "Simple assignment\n",
      "Accuracy: 0.8287037037037037\n",
      "F1-score: 0.45316455696202534\n",
      "Naive Bayes\n",
      "Accuracy: 0.6805555555555556\n",
      "F1-score: 0.4433613445378151\n"
     ]
    }
   ],
   "source": [
    "for questions in question_type:\n",
    "    for classification_type in dataset_types:\n",
    "        validation, test = pd.read_csv('validacao.csv'), pd.read_csv('teste.csv')\n",
    "\n",
    "        if questions == 'standard':\n",
    "            train = pd.read_csv('treinamento.csv')\n",
    "\n",
    "            if language == 'english':\n",
    "                train = train[['abstract', 'question_en_origin', 'answer_en_origin', \n",
    "                                            'question_meaningful', 'corpus', 'answer_in_text', 'answer_en_validate']]\n",
    "\n",
    "                train.rename(columns={'question_en_origin': 'question', 'answer_en_origin': 'answer',\n",
    "                                            'question_meaningful': 'label'}, inplace=True)\n",
    "\n",
    "            if language == 'portuguese':\n",
    "                train = train[['abstract', 'question_pt_origin', 'answer_pt_origin', \n",
    "                                            'question_meaningful', 'corpus', 'answer_in_text', 'answer_pt_validate']]\n",
    "\n",
    "                train.rename(columns={'question_pt_origin': 'question', 'answer_pt_origin': 'answer',\n",
    "                                            'question_meaningful': 'label'}, inplace=True)\n",
    "\n",
    "        if questions == 'human_paraphrase':\n",
    "            train = pd.read_csv('treinamento.csv')\n",
    "\n",
    "            if language == 'english':\n",
    "\n",
    "                # acrescentando paráfrases humanas\n",
    "                trein_1 = train[['abstract', 'question_en_origin', 'answer_en_origin', \n",
    "                                        'question_meaningful', 'corpus' , 'answer_in_text']]\n",
    "                trein_2 = train[['abstract', 'question_en_origin', 'answer_en_validate', \n",
    "                                        'question_meaningful', 'corpus', 'answer_in_text']]\n",
    "                trein_3 = train[['abstract', 'question_en_paraphase', 'answer_en_origin', \n",
    "                                        'question_meaningful', 'corpus', 'answer_in_text']]\n",
    "                trein_4 = train[['abstract', 'question_en_paraphase', 'answer_en_validate', \n",
    "                                        'question_meaningful', 'corpus', 'answer_in_text']]\n",
    "\n",
    "                trein_1.columns = ['abstract', 'question', 'answer', 'label', 'corpus', 'answer_in_text']\n",
    "                trein_2.columns = ['abstract', 'question', 'answer', 'label', 'corpus', 'answer_in_text']\n",
    "                trein_3.columns = ['abstract', 'question', 'answer', 'label', 'corpus', 'answer_in_text']\n",
    "                trein_4.columns = ['abstract', 'question', 'answer', 'label', 'corpus', 'answer_in_text']\n",
    "\n",
    "                frames = [trein_1, trein_2, trein_3, trein_4]\n",
    "\n",
    "                train = pd.concat(frames)\n",
    "\n",
    "            if language == 'portuguese':\n",
    "\n",
    "                # acrescentando paráfrases humanas\n",
    "                trein_1 = train[['abstract', 'question_pt_origin', 'answer_pt_origin', \n",
    "                                        'question_meaningful', 'corpus', 'answer_in_text']]\n",
    "                trein_2 = train[['abstract', 'question_pt_origin', 'answer_pt_validate', \n",
    "                                        'question_meaningful', 'corpus', 'answer_in_text']]\n",
    "                trein_3 = train[['abstract', 'question_pt_paraphase', 'answer_pt_origin', \n",
    "                                        'question_meaningful', 'corpus', 'answer_in_text']]\n",
    "                trein_4 = train[['abstract', 'question_pt_paraphase', 'answer_pt_validate', \n",
    "                                        'question_meaningful', 'corpus', 'answer_in_text']]\n",
    "\n",
    "                trein_1.columns = ['abstract', 'question', 'answer', 'label', 'corpus', 'answer_in_text']\n",
    "                trein_2.columns = ['abstract', 'question', 'answer', 'label', 'corpus', 'answer_in_text']\n",
    "                trein_3.columns = ['abstract', 'question', 'answer', 'label', 'corpus', 'answer_in_text']\n",
    "                trein_4.columns = ['abstract', 'question', 'answer', 'label', 'corpus', 'answer_in_text']\n",
    "\n",
    "                frames = [trein_1, trein_2, trein_3, trein_4]\n",
    "\n",
    "                train = pd.concat(frames)\n",
    "\n",
    "        if questions == 'automatic_paraphrase':\n",
    "\n",
    "            if language == 'english':\n",
    "                df = pd.read_csv('parafrases_en_pegasus.csv')\n",
    "\n",
    "                lst = []\n",
    "\n",
    "                for i in range(1,7):\n",
    "                    qu = 'question_' + str(i)\n",
    "                    for j in range(1,7):\n",
    "                        ans = 'answer_' + str(j)\n",
    "                        data = df[['abstract', 'labels', qu, ans, 'corpus', 'answer_in_text']]\n",
    "                        data = data.rename(columns={qu: 'question', ans: 'answer'})\n",
    "                        lst.append(data)\n",
    "\n",
    "                train = pd.concat(lst)\n",
    "\n",
    "                train.rename(columns={'labels': 'label'}, inplace=True)\n",
    "                validation.rename(columns={'labels': 'label'}, inplace=True)\n",
    "                test.rename(columns={'labels': 'label'}, inplace=True)\n",
    "                \n",
    "            if language == 'portuguese':\n",
    "                df = pd.read_csv('parafrases_pt.csv')\n",
    "\n",
    "                lst = []\n",
    "\n",
    "                for i in range(1,7):\n",
    "                    qu = 'question_' + str(i)\n",
    "                    for j in range(1,7):\n",
    "                        ans = 'answer_' + str(j)\n",
    "                        data = df[['abstract', 'labels', qu, ans, 'corpus', 'answer_in_text']]\n",
    "                        data = data.rename(columns={qu: 'question', ans: 'answer'})\n",
    "                        lst.append(data)\n",
    "\n",
    "                train = pd.concat(lst)\n",
    "                \n",
    "                train.rename(columns={'labels': 'label'}, inplace=True)\n",
    "                validation.rename(columns={'labels': 'label'}, inplace=True)\n",
    "                test.rename(columns={'labels': 'label'}, inplace=True)\n",
    "\n",
    "\n",
    "        if language == 'english':\n",
    "            validation = validation[['corpus', 'abstract', 'question_en_origin', 'answer_en_origin', \n",
    "                                   'question_meaningful', 'answer_in_text', 'answer_en_validate']]\n",
    "            test = test[['corpus', 'abstract', 'question_en_origin', 'answer_en_origin', \n",
    "                            'question_meaningful', 'answer_in_text', 'answer_en_validate']]\n",
    "            validation.rename(columns={'question_en_origin': 'question', 'answer_en_origin': 'answer',\n",
    "                                          'question_meaningful': 'label'}, inplace=True)\n",
    "            test.rename(columns={'question_en_origin': 'question', 'answer_en_origin': 'answer',\n",
    "                                      'question_meaningful': 'label'}, inplace=True)\n",
    "\n",
    "        if language == 'portuguese':\n",
    "            validation = validation[['corpus', 'abstract', 'question_pt_origin', 'answer_pt_origin', \n",
    "                                   'question_meaningful', 'answer_in_text', 'answer_pt_validate']]\n",
    "            test = test[['corpus', 'abstract', 'question_pt_origin', 'answer_pt_origin', \n",
    "                            'question_meaningful', 'answer_in_text', 'answer_pt_validate']]\n",
    "            validation.rename(columns={'question_pt_origin': 'question', 'answer_pt_origin': 'answer',\n",
    "                                          'question_meaningful': 'label'}, inplace=True)\n",
    "            test.rename(columns={'question_pt_origin': 'question', 'answer_pt_origin': 'answer',\n",
    "                                      'question_meaningful': 'label'}, inplace=True)\n",
    "        \n",
    "\n",
    "        ## Cleaning the data\n",
    "\n",
    "        # tirar as tags\n",
    "        train = train.replace('<[^<]+?>', ' ', regex=True)\n",
    "        validation = validation.replace('<[^<]+?>', ' ', regex=True)\n",
    "        test = test.replace('<[^<]+?>', ' ', regex=True)\n",
    "\n",
    "        # tirar quebra de linha, parágrafo\n",
    "        train.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "        validation.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "        test.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
    "\n",
    "        # remove front and ending blank spaces\n",
    "        train = train.replace({\"^\\s*|\\s*$\":\"\"}, regex=True)\n",
    "        validation = validation.replace({\"^\\s*|\\s*$\":\"\"}, regex=True)\n",
    "        test = test.replace({\"^\\s*|\\s*$\":\"\"}, regex=True)\n",
    "\n",
    "        # remove multiple whitespace\n",
    "        train = train.replace('\\s+', ' ', regex=True)\n",
    "        validation = validation.replace('\\s+', ' ', regex=True)\n",
    "        test = test.replace('\\s+', ' ', regex=True)\n",
    "\n",
    "        #dropna\n",
    "        train = train.dropna()\n",
    "        validation = validation.dropna()\n",
    "        test = test.dropna()\n",
    "\n",
    "        ## Context\n",
    "        train['text'] = train['abstract'] + \" </s> \" + train['question']\n",
    "        validation['text'] = validation['abstract'] + \" </s> \" + validation['question']\n",
    "        test['text'] = test['abstract'] + \" </s> \" + test['question']\n",
    "\n",
    "        test['label'].value_counts()\n",
    "\n",
    "        # create classfication labels\n",
    "\n",
    "        if classification_type == 'rigid':\n",
    "            train = train.loc[(train['label'] == 1) | (train['label'] == 5)]\n",
    "            validation = validation.loc[(validation['label'] == 1) | (validation['label'] == 5)]\n",
    "            test = test.loc[(test['label'] == 1) | (test['label'] == 5)]\n",
    "\n",
    "        if classification_type == 'divided':\n",
    "            train = train.loc[(train['label'] < threshold) | (train['label'] > threshold)]\n",
    "            validation = validation.loc[(validation['label'] < threshold) | (validation['label'] > threshold)]\n",
    "            test = test.loc[(test['label'] < threshold) | (test['label'] > threshold)]\n",
    "\n",
    "        train['label'] = train['label'].apply(lambda x: 1 if x > threshold else 0)\n",
    "        validation['label'] = validation['label'].apply(lambda x: 1 if x > threshold else 0)\n",
    "        test['label'] = test['label'].apply(lambda x: 1 if x > threshold else 0)\n",
    "\n",
    "        from datasets import Dataset\n",
    "\n",
    "        train_dataset = Dataset.from_dict(train)\n",
    "        validation_dataset = Dataset.from_dict(validation)\n",
    "        test_dataset = Dataset.from_dict(test)\n",
    "\n",
    "        my_dataset_dict = datasets.DatasetDict({\"train\":train_dataset, \n",
    "                                                'validation': validation_dataset, \"test\":test_dataset})\n",
    "\n",
    "        my_dataset_dict\n",
    "\n",
    "        #Most common value is 1, so we use to predict the results in the test set\n",
    "\n",
    "        from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "        test['simple_assignment'] = 1\n",
    "\n",
    "        print('Test:', questions, classification_type)\n",
    "        print('Simple assignment')\n",
    "        print('Accuracy:', accuracy_score(test['label'], test['simple_assignment']))\n",
    "        print('F1-score:', f1_score(test['label'], test['simple_assignment'], average='macro'))\n",
    "        \n",
    "        # Naive Bayes\n",
    "        # Build the model\n",
    "\n",
    "        from sklearn.naive_bayes import MultinomialNB\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "        model = make_pipeline(TfidfVectorizer(), MultinomialNB())# Train the model using the training data\n",
    "        model.fit(train['text'].values, train['label'].values)# Predict the categories of the test data\n",
    "        \n",
    "        test['bayes'] = model.predict(test['text'].values)\n",
    "        \n",
    "        print('Naive Bayes')\n",
    "        print('Accuracy:', accuracy_score(test['label'], test['bayes']))\n",
    "        print('F1-score:', f1_score(test['label'], test['bayes'], average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1775c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
